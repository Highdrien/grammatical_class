name: experiment
save_experiment: true

# data options
data:
  path: data                  # data path
  language: English           # Language of data
  sequence_length: 10         # length of sequence
  pad: <PAD>                  # pad caracter to make padding
  unk: <UNK>                  # unknow caracter
  sequence_function: dummy    # function to split sentences to sequences
  indexes: [1, 3, 5]          # index of information that will be recupered: 
                                # indexes representation: 
                                # 0: id       1: word    2: lemma   3: pos   4: unk
                                # 5: morphy   6: syntax  7: unk     8: unk   9: unk 
  vocab:
    path: dictionary          # path to save the vocabulary
    unk_rate: 0.01            # rate to replace a knowing word by <UNK>
    save: false               # save the vocabulary or load it from data.voc.path

# model options
model:
  model_name: lstm            # choose between lstm or bert
  task: get_pos               # choose between get_pos and get_morphy
  num_classes: 19
  lstm:
    lstm_hidd_size_1: 64     # first hidden layer size of LSTM layers
    lstm_hidd_size_2: Null      # second hidden layer size of LSTM layers. Put Null if you want only one LSTM Module
    fc_hidd_size: []    # hidden layers of fully connected layers. Do not specify the last layers,
                                # so [] means just one fc layers with output = num_classes
    embedding_size: 64       # embedding size
    bidirectional: true
    activation: relu          # activation function btw layer: relu or sigmoid
  bert:
    hidd_size: 100
    pretrained_model_name: bert-base-uncased

# learning options
learning:
  loss: crossentropy          # loss
  optimizer: adam             # optimizer
  learning_rate: 0.01         # learning rate
  milesstone: [5, 15]         # gradient decay at epoch 5 and 15
  gamma: 0.1                  # learning rate will be multiplicate by 0.1 at epochs 5 and 15
  epochs: 10                  # number of epochs
  batch_size: 2048            # batch size
  shuffle: true
  drop_last: true
  save_checkpoint: true
  