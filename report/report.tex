\documentclass[a4paper]{article}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}   % pour les images
\usepackage{hyperref}   % pour les références
\usepackage{amssymb}    % pour les symboles de maths comme \mathbb{R}
\usepackage{mathtools}  % pour rajouter \text dans un environment math
\usepackage{subcaption} % pour les subfigures
\usepackage{float}      % truc pour mettre les images à la bonne place
\usepackage[style=ieee]{biblatex}
\addbibresource{ref.bib}
\bibliography{ref.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,   % Couleur des liens internes (table des matières, références)
    citecolor=green,  % Couleur des liens vers les références bibliographiques
    filecolor=magenta,% Couleur des liens vers les fichiers
    urlcolor=blue     % Couleur des liens vers les URL
}


\title{Rapport PSTALN}
\author{Cléa Han, Yanis Labeyrie et Adrien Zabban}
\date{janvier 2024}

\begin{document}

\maketitle
\bigskip
\tableofcontents
\newpage

\section{Introduction}

Le but de ce projet est de faire un modèle de langage capable de prédire les morphologies des mots d'une phrase
(\textit{morphy}), comme le montre la Figure~\ref{fig: example morphy}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{morphy.png}
    \caption{Tag morphologique d'une phrase portugaise et sa traduction en espagnol.
    Image tirée de~\cite{malaviya-etal-2018-neural}}
    \label{fig: example morphy}
\end{figure}    

Nous allons aussi nous demander si est-ce que le fait d'ajouter un prétraitement de la phrase va améliorer
les performances. L'idée de ce prétraitement est de faire prédire le balisage de séquence
prototypique (\textit{pos}) des mots, comme le montre la Figure~\ref{fig: example pos}. L'ensemble des \textit{pos} 
sont recensé dans les Annexes (section~\ref{sec: Annexes}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{pos.png}
    \caption{La tâche d'étiquetage des parties du discours : la mise en correspondance des mots d'entrée 
    $x_1, x_2,..., x_n$ avec les étiquettes \textit{pos} de sortie $y_1, y_2,..., y_n$.
    Image tirée de~\cite{pos}}
    \label{fig: example pos}
\end{figure} 

\section{Données}

Nous avons utilisé le dataset Universal Dependencies 2.13 ~\cite{11234/1-5287}, qui comporte 146 langues dont l'anglais
et le français. Dans ce projet, nous nous sommes seulement concentrés sur le français.
Le dataset en français contient un ensemble de 47498 phrases, avec 849476 mots, et 76048 mots uniques.
Ce dataset possède des phrases, la liste de mots composant ses phrases,
et les \textit{pos} et \textit{morphy} qui sont associés à chacun de ses mots. Au total, nous avons recensé $19$ \textit{pos} et 
$28$ \textit{morphy} différents.

\subsection{Padding}

Les phrases données au modèle doivent être toutes exactement de la même longueur pendant l'entraînement. 
Notons $K$ la longueur des séquences\footnote{Nous appelons séquence les suites de mots de même taille.}. Nous créons donc des 
tokens <PAD> pour équilibrer les longueurs des phrases.
Nous avons choisi d'utiliser une méthode naïve pour créer nos séquences qui consiste à couper chaque phrase pour former les séquences
de $K$ mots, et de rajouter si besoin des tokens <PAD> pour terminer la dernière séquence. Nous avons choisi de prendre $K=10$.

\subsection{Gestion des mots inconnus}

Nous avons, avant l'entraînement du modèle, appris un vocabulaire de mots qui fait la correspondance entre les mots et un nombre unique.
Le vocabulaire a été fait seulement sur les mots qui sont dans la base de données d'entraînement. C'est donc pour cela qu'il peut
arriver que des mots de la base de données de validation ou de teste peuvent ne pas être reconnue par le vocabulaire et donc le modèle.
Pour gérer ces mots inconnus, nous avons décidé de leur attribuer le token particulier : <UNK>. Pour que le modèle
apprenne l'embedding de ce mot, il faut alors rajouter artificiellement des mots inconnus dans le corpus d'entraînement. Nous avons
donc, pour chaque mot de ce corpus, remplacé par <UNK> avec une probabilité de $1\%$.


\subsection{Encodage des labels}

Pour encoder les \textit{pos}, nous avons seulement créé une liste de tous les \textit{pos} et avons encodé les \textit{pos} avec 
leurs indices dans la liste.
Nous avons aussi ajouté le \textit{pos} <PAD> pour que le token de padding soit catégorisé dans ce \textit{pos}.


Pour les \textit{morphy}, cela a été plus compliqué, car un mot peut avoir plusieurs \textit{morphy} associé, avec des valeurs
différentes. Nous avons décidé d'encoder une suite de \textit{morphy} par une liste de nombre de longueurs $28$ et les éléments
sont les indices des possibilités de chaque
\textit{morphy}. Par exemple : le label \textit{Emph=No|Number=Sing|Person=1|PronType=Prs} est encodé par la liste ci-dessous :

[0, 0, 1, 0, 1, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

Comme pour le \textit{pos}, nous avons aussi ajouté un \textit{morphy} pour le padding. Étant donné que le \textit{morphy} qui possède
le plus de possibilités en possède 13, quand nous encodons les labels en one-hot, nous avons un tensor de shape
$(19)$ pour les \textit{pos} et un tensor de shape $(28, 13)$ pour les \textit{morphy}.


\section{Modèle}

\subsection{Architecture des \textit{pos}}

Le modèle, que nous appelons \textit{GET\_POS}, est constitué d'une couche d'embedding pour apprendre les plongements
des mots. Les données passent alors dans une couche LSTM bidirectionnel, puis dans une couche dense (fully connected layers),
avec une sortie à 19 éléments représentant les probabilité de chaque classe \textit{pos}. Nous avons utilisé du 
dropout sur les neurones des couches LSTM et dense, avec un taux d'oublie de $1\%$. Entre ces 3 couches, nous avons 
aussi ajouté la fonction d'activation ReLU~\cite{DBLP:journals/corr/abs-1803-08375}. Nous avons utilisé la CrossentropyLoss 
comme fonction de coût, l'optimizer Adam~\cite{kingma2014adam}. Ce modèle est représenté sur la Figure~\ref{fig: model getpos}.
Nous avons donc en entrée une matrice de taille $B \times K$, contenant l'indice des mots, où $B$ est la taille du batch.
 Et le modèle retourne un tensor de taille $B \times K \times 19$, contenant les probabilités des \textit{pos} pour chaque mot.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{get_pos.png}
    \caption{Modèle \textit{GET\_POS}}
    \label{fig: model getpos}
\end{figure} 

\subsection{Architecture des \textit{morphy}}

Pour la tâche de prédiction des traits morphologiques, la difficulté est supérieure. En effet, pour cette tâche 
il faut pour chaque mot de la phrase prédire à la fois son rôle dans la phrase (Verbe, Nom, etc.) mais aussi 
pour chacune de ses classes prédire des attributs (singulier, pluriel, etc.). Nous avons donc créé plusieurs 
architecture différente que nous allons présenter. Nous avons aussi dû modifier la fonction de coût par 
la moyenne des crossentropy sur les 28 \textit{morphy} différents. Nous avons utilisé l'optimizer Adam~\cite{kingma2014adam},
et avons ajouté un gradient decay, qui fait que le learning rate est divisé par deux lors des epochs 10 et 20.

\subsubsection{Modèle \textit{SUPERTAG}}

Le modèle commence par une couche d'embedding pour deux couches de LSTM bidirectionnel. Ensuite, on fait passer les données 
dans 3 couches denses cachées dont la dernière contient $364$ neurones
\footnote{Nous avons pris $364$ neurones car $364 = 28 \times 13$.}
Avant que nos données sortent du modèle, elles ont une
taille de $B \times B \times 364$. On \textit{reshape} alors les données pour avoir une sortie de taille 
$B \times B \times 28 \times 13$. La Figure~\ref{fig: model supertag} représente ce modèle.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{get_morphy_supertag.png}
    \caption{Modèle \textit{SUPERTAG}}
    \label{fig: model supertag}
\end{figure}

\subsubsection{Modèle \textit{SEPARATE}}

On reprend le même début que le modèle \textit{SUPERTAG} et l'on change la dernière couche. Au lieu de prédire tous les 
\textit{morphy} en même temps avec une seule couche dense, on va avoir 28 couche dense (une par \textit{morphy}). Chaque couche
va alors prédire les probabilités liées à son \textit{morphy}. On va alors ajouter, à chaque prédiction des couches, des $- \infty$
pour obtenir un vecteur de taille $13$ pour tous les mots
\footnote{On mets $-\infty$ car cela permet d'avoir une probabilité de $0$ après le softmax.}.
On va ensuite concaténer tous les résultats avec une sortie de 
taille $B \times B \times 28 \times 13$. La Figure~\ref{fig: model separate} représente ce modèle.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{get_morphy_separate.png}
    \caption{Modèle \textit{SEPARATE}.}
    \label{fig: model separate}
\end{figure}

\subsubsection{Modèle \textit{FUSION}}

L'idée de ce modèle est d'utiliser la prédiction du modèle \textit{GET\_POS} pour aider le modèle \textit{SEPARATE}.
On fait passer les données dans les 2 couches de LSTM, et en sortie on va concaténer la sortie du LSTM avec la sortie
d'un modèle \textit{GET\_POS} pré-entraînemé.
La Figure~\ref{fig: model fusion} représente ce modèle.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{get_morphy_fusion.png}
    \caption{Modèle \textit{FUSION}}
    \label{fig: model fusion}
\end{figure}


\section{Métriques}

Pour pouvoir mesurer les performances de ces modèles, nous avons mis en place plusieurs métriques. Pour la prédiction de 
\textit{pos}, nous avons utilisé l'accuracy micro et macro. La première nous donne l'accuracy de la prédiction et la deuxième
nous donne la moyenne de l'accuracy sur chacune des classes. 

Pour la prédiction de \textit{morphy}, nous avons aussi utilisé l'accuracy micro, et nous avons aussi implémenté une 
métrique qu'on a appelée \textit{all good}, qui fait la moyenne des mots dont la prédiction de tous les \textit{morphy}
sont juste. Si la métrique vaut $0.2$, cela veut dire qu'il y a 1 mot sur 5, qui dont la prédiction est totalement bonne.

\section{Baseline}

Nous avons implementée une baseline afin d'effectuer des comparaisons de performances avec nos modèles. \\

La baseline s'appuie sur un modèle simple où un dictionnaire est créé et parcourt l'ensemble des données d'entraînements afin de récolter tous les mots qu'il n'a jamais croisé et leur label. Ainsi, le dictionnaire indique le vocabulaire du texte en lui assignant le premier label croisé lors du parcours du dataset d'entraînement. \\

Cette baseline s'appuie sur une logique "naïve" en quelque sorte, et utilise les mêmes métriques que nos modèles. \\

Notre baseline témoigne d'une performance relativement élevée, qui sera mise en contraste avec les performances des autres modèles décrits précédemment. 

\section{Résultats}

\subsection{Résultats pour la prédiction de \textit{pos}}
Nous avons donc lancé un entraînement de notre réseau LSTM-Bidirectionel sur 30 epochs pour le \textit{pos}-tagging et 
nous avons obtenu une accuracy de validation d'environ 90\% ce qui dénote que le réseau a réussi à apprendre 
correctement cette tâche d'étiquetage, comme le montre la figure suivante :

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/get_pos_French/crossentropy.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/get_pos_French/acc micro.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/get_pos_French/acc macro.png}
    \end{subfigure}
    \caption{Valeurs de la loss et des métriques d'entraînement et de validation en fonction des epochs.}
\end{figure}

Les courbes de loss et d'accuracy décrivent un apprentissage efficient dès la 5e epoch. À partir de la 5e epoch, l'apprentissage
stagne et si on s'étend à plus de 20 epochs, on risque de faire du overfitting, car l'écart entre les performances de l'entraînement
et les performances de la validation commence à se creuser. 
L'accuracy micro est relativement meilleure que l'accuracy macro. D'autre part les performances d'accuracy macro sont moins stables que celle de l'accuracy micro. 

On constate que la valeur de l'accuracy de validation est un peu plus faible que celle d'entraînement, cela 
s'explique par plusieurs facteurs, notamment la difficulté du modèle à généraliser aux nouveaux mots inconnus.


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Nom du modèle & crossentropy & accuracy micro & accuracy macro \\
        \hline
        \textit{GET\_POS} & 0.204 & 0.944 & 0.816\\
        \hline
    \end{tabular}
    \caption{Résultats du modèle \textit{GET\_POS} sur la base de données de teste.}
    \label{tab:test getpos}
\end{table}

Les résultats du tableau\ref{tab:test getpos} témoignent d'une performance satisfaisante du modèle POS avec une accuracy micro de 94,4\%. Néanmoins, si on souhaite d'intéresser à l'accuracy micro, nous atteignons une accuracy de 81,6\%.

\subsection{Résultats la prédiction des \textit{morphy}}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/supertag/crossentropy.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/supertag/acc micro.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/supertag/allgood.png}
    \end{subfigure}
    \caption{Valeurs de la loss et des métriques d'entraînement et de validation en fonction des epochs 
            pour le modèle \textit{SUPERTAG}.}
    \label{fig: results supertag}
\end{figure}

Les courbes de loss témoignent d'un entraînement efficient. Cependant, les courbes d'accuracy montrent que les performances ne
s'améliorent pas de la même manière que la loss. Les courbes témoignent même d'une amélioration linéaire plutôt qu'exponentielle.
D'autre part, si on considère les performances selon l'accuracy micro, il n'y a pas d'écart qui
se creuse au fil des epochs entre les deux courbes, donc il n'y a pas de overfitting selon cette métrique. On peut expliquer la meilleure performance
de la validation par rapport à l'entraînement par l'utilisation du dropout dans l'apprentissage. Cette méthode est présente lors
de l'entraînement, ce qui va naturellement baisser ses performances. 
Néanmoins, l'accuracy micro est plus stable que celle de l'accuracy \textit{allgood}, où l'écart entre validation et entraînement se creuse au fil des epochs. L'amélioration des performances
n'est donc pas égale au fil des dernières epochs entre les différentes classes. Le modèle a donc tendance à surapprendre si on considère l'accuracy \textit{allgood}. 


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/separate/crossentropy.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/separate/acc micro.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/separate/allgood.png}
    \end{subfigure}
    \caption{Valeurs de la loss et des métriques d'entraînement et de validation en fonction des epochs 
            pour le modèle \textit{SEPARATE}.}
    \label{fig: results separate}
\end{figure}

La modèle \textit{SEPARATE} témoigne d'une bonne performance selon l'accurcy micro, où les performances de l'entraînement et de la validation corrèlent tout du long. Cet apprentissage est également exponentielle d'après l'allure de la loss et de l'accuracy micro. Néanmoins, l'écart entre performances d'entraînement et de validation au niveau de la loss se creuse au fil des epochs, ce qui signifie qu'il risque d'y avoir du surapprentissage. Cependant, l'accuracy \textit{allgood} reste presque "linéaire" avec un écart entre
validation et entraînement qui se creuse au fil des epochs tout en étant de plus en plus instable. Le modèle ne traduit de performances stables et correctes si l'on ne considère que la loss et l'accuracy micro, en mettant de côté les performances selon l'accuracy \textit{allgood}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/fusion/crossentropy.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/fusion/acc micro.png}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../logs/fusion/allgood.png}
    \end{subfigure}
    \caption{Valeurs de la loss et des métriques d'entraînement et de validation en fonction des epochs 
            pour le modèle \textit{FUSION}.}
    \label{fig: results fusion}
\end{figure}

Le modèle \textit{FUSION} témoigne d'un apprentissage similaire au modèle précédent \textit{SEPARATE}. Cependant, les performances considérées selon l'accuracy \textit{allgood} adoptent une tendance relativement plus exponentielle, ce qui permet de mieux le corréler avec les performances de la loss et de l'accuracy micro. D'autre part, on retrouve le phénomène de performances de validation meilleures que celles de l'entraînement pour l'accuracy \textit{allgood}, avec relativement moins d'instabilité que le modèle précédent, mais l'instabilité subsiste. Ce modèle \textit{FUSION} témoigne d'une amélioration vis-à-vis du modèle \textit{SEPARATE}.


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Nom du modèle & crossentropy & accuracy micro & all good\\
         \hline
         \textit{BASELINE}& - & 0.980 & 0.791 \\
         \hline
         \textit{SUPERTAG}& 1.700 & 0.436 & 0.002\\
         \hline
         \textit{SEPARATE}& 1.70 & 0.893 & 0.046\\
         \hline
         \textit{FUSION}& 1.698 & 0.884 & 0.154 \\
         \hline
    \end{tabular}
    \caption{Résultats de test sur la prédiction des \textit{morphy}}
    \label{tab: test morphy}
\end{table}

Les résultats du tableau\ref{tab: test morphy} mettent en comparaison les résultats de performances entre les modèles de \textit{BASELINE}, \textit{SUPERTAG}, \textit{SEPARATE} et \textit{FUSION}. Le modèle baseline indique une performance d'accuracy de 98\% 

Entre les trois autres modèles, c'est le modèle \textit{SEPARATE} qui performe le mieux en terme d'accuracy micro, cependant, c'est le modèle \textit{FUSION} qui performe le mieux en terme d'accuracy allgood. Il y a une amélioration de l'accuracy allgood d'un facteur 3 par rapport au modèle \textit{SEPARATE} avec seulement une baisse d'à peine 1\% d'accuracy micro. Cette relative meilleure performance du modèle \textit{FUSION} est notamment confirmée avec l'analyse des performances graphiques du modèle. 

Néanmoins, ces modèles ne permettent pas de dépasser les performances de la baseline, alors que les modèles implémentés ont une significative complexité plus importante que celle de la baseline. Ce qui se peut s'expliquer que la baseline s'est appuyée sur un dataset trop petit et des données biaisées, où la coïncidence a sûrement dû jouer. 

\section{Inférences}

\newpage

\section{Conclusion}

Nous avons exploré deux tâches principales : l'étiquetage des parties du discours (\textit{pos}) et la prédiction des traits morphologiques (\textit{morphy}). 

Le modèle \textit{GET\_POS} a démontré des performances impressionnantes dans la prédiction des parties du discours, avec une accuracy micro de 94.4\% et une accuracy macro de 81.6\% sur le jeu de test. Ces résultats témoignent de la capacité du modèle à apprendre efficacement la structure grammaticale des phrases en français.

Pour la tâche de prédiction des traits morphologiques, trois architectures de modèles ont été développées : \textit{SUPERTAG}, \textit{SEPARATE}, et \textit{FUSION}. Chacun de ces modèles a présenté des performances variées. Le modèle \textit{SEPARATE} a obtenu une accuracy micro de 89.3\% et une métrique \textit{all good} de 4.6\%, montrant sa capacité à prédire les traits morphologiques pour chaque classe. Cependant, le modèle \textit{FUSION} a montré une instabilité lors de l'entraînement, ce qui peut nécessiter des ajustements pour améliorer ses performances.

Les résultats obtenus ouvrent des perspectives intéressantes pour l'amélioration des modèles de langage, en particulier dans le contexte du français. Des ajustements futurs pourraient inclure des stratégies plus sophistiquées pour la gestion des mots inconnus et des expériences avec d'autres architectures de modèles.

Ce projet a donc permis d'explorer différentes facettes de la modélisation du langage, de la prédiction des parties du discours à la prédiction des traits morphologiques. Les résultats obtenus constituent une base solide pour des développements futurs dans le domaine de la compréhension automatique du langage naturel en français.

\section{Annexes}
\label{sec: Annexes}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{all_pos.png}
    \caption{Liste des labels \textit{pos}. Image tirée de~\cite{pos}}
\end{figure}   

\newpage

\printbibliography


\end{document}